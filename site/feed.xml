<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
  <title>AI Changelog Insights</title>
  <link>https://ai-changelog-insights.github.io</link>
  <description>Daily AI updates, summarized for developers.</description>
  <atom:link href="https://ai-changelog-insights.github.io/feed.xml" rel="self" type="application/rss+xml" />
  <language>en-us</language>
  <lastBuildDate>Wed, 25 Feb 2026 17:48:12 GMT</lastBuildDate>
  
  <item>
    <title>langchain - langchain-core 1.2.16</title>
    <link>https://github.com/langchain-ai/langchain</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release fixes a critical issue with tool chunk ID handling in the merge functionality and includes a minor release bump.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Fix empty tool chunk IDs in merge</strong>: Treats empty tool chunk IDs as missing during merge operations, preventing potential data loss or incorrect merging behavior in complex tool call chains.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.messages import HumanMessage
from langchain_core.tools import Tool

# Example showing proper tool chunk handling
class MyTool(Tool):
    """Example tool that demonstrates proper chunk ID handling."""
    
    def __init__(self):
        super().__init__(
            name="example",
            description="Demonstrates proper chunk ID handling",
            func=lambda x: x
        )

# Usage in a chain (simplified)
message = HumanMessage(content="Run the example tool")
# The merge functionality will now properly handle empty chunk IDs
# No code changes needed - the fix is automatic</code></pre>
    ]]></description>
    <guid isPermaLink="false">langchain-ai/langchain-2026-02-25</guid>
    <pubDate>Wed, 25 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>anthropic-sdk-python - 0.84.0 - MCP Tools, Prompts & Resources Support</title>
    <link>https://github.com/anthropics/anthropic-sdk-python</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release adds comprehensive MCP (Model Context Protocol) conversion helpers, removes the publishing section from CLI target, and changes array format to brackets. It also includes internal improvements to SSE classes and proxy testing resilience.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>MCP Conversion Helpers</strong>: New helpers for converting between MCP tools, prompts, and resources make it easier to integrate with MCP servers and clients</li>
<li><strong>Array Format Change</strong>: Changed array_format to brackets for better compatibility with API specifications</li>
<li><strong>CLI Cleanup</strong>: Removed publishing section from CLI target to streamline the command-line interface</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from anthropic import Anthropic

# Using new MCP conversion helpers
anthropic = Anthropic()

# Convert MCP tool to SDK format
mcp_tool = anthropic.mcp.convert_tool_to_sdk(mcp_tool_definition)

# Convert SDK tool to MCP format
mcp_definition = anthropic.mcp.convert_sdk_to_mcp(tool_definition)

# Convert prompts and resources
mcp_prompt = anthropic.mcp.convert_prompt_to_mcp(prompt_definition)
mcp_resource = anthropic.mcp.convert_resource_to_mcp(resource_definition)</code></pre>
    ]]></description>
    <guid isPermaLink="false">anthropics/anthropic-sdk-python-2026-02-25</guid>
    <pubDate>Wed, 25 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>ragflow - v0.24.0</title>
    <link>https://github.com/infiniflow/ragflow</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>RAGFlow 0.24.0 introduces significant enhancements across memory management, dataset handling, agent capabilities, and ecosystem integrations. The release focuses on improving observability, developer experience, and system governance.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Memory Management</strong>: Memory extraction logs are now visible in the console, providing full traceability of extraction, embedding, and storage operations. RESTful APIs and Python SDK enable comprehensive CRUD operations and intelligent retrieval of memories.</li>
<li><strong>Dataset Management</strong>: Batch metadata management improves user experience when configuring application metadata. Table of Contents (ToC) has been renamed to PageIndex for clarity.</li>
<li><strong>Agent Chat Management</strong>: New multi-Sandbox mechanism supports local gVisor and Alibaba Cloud with mainstream Sandbox API compatibility. Chat interface retains sessions and dialogue history for better conversation continuity.</li>
<li><strong>Chat Optimization</strong>: New 'Thinking' mode replaces previous 'Reasoning' configuration, with optimized retrieval strategies for deep-research scenarios that enhance recall accuracy.</li>
<li><strong>Admin Enhancements</strong>: Support for multiple admin accounts and model connection testing when adding new models improves system administration and configuration reliability.</li>
<li><strong>Ecosystem Expansion</strong>: OceanBase database support provides an alternative to MySQL. PaddleOCR-VL integration expands multimodal processing capabilities. New data source integrations include Zendesk and Bitbucket.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from ragflow.memory import MemoryClient

# Initialize Memory client
client = MemoryClient()

# Create a new memory
memory = client.create_memory(name="Project X", description="Research project data")

# Add memory entry
entry = client.add_memory_entry(
    memory_id=memory.id,
    content="This is the content for memory entry",
    metadata={"source": "document.pdf", "page": 5}
)

# Retrieve memories by keyword
results = client.search_memories(keyword="research")
print(f"Found {len(results)} memories")

# Get memory extraction logs
logs = client.get_memory_logs(memory_id=memory.id)
for log in logs:
    print(f"{log.timestamp}: {log.message}")</code></pre>
    ]]></description>
    <guid isPermaLink="false">infiniflow/ragflow-2026-02-25</guid>
    <pubDate>Wed, 25 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>mindsdb - v26.0.0 - Major Release with Breaking Changes</title>
    <link>https://github.com/mindsdb/mindsdb</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release introduces significant improvements across SQL capabilities, integrations, Knowledge Bases, and security. Key highlights include enhanced GROUP BY WITH ROLLUP functionality, static README integration from GitHub, pgvector as default Knowledge Base store in Docker, and numerous security upgrades to dependencies.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Enhanced SQL GROUP BY WITH ROLLUP</strong>: Fixed reliability issues with GROUP BY WITH ROLLUP, making it more robust for complex analytical queries and rollup operations.</li>
<li><strong>Static README Integration</strong>: README files for integrations are now pulled statically from GitHub, reducing local storage requirements and ensuring documentation stays current.</li>
<li><strong>pgvector Default for Knowledge Bases</strong>: Switched default Knowledge Base store in Docker-compose to pgvector for enhanced performance in vector operations and similarity searches.</li>
<li><strong>Security Upgrades</strong>: Updated numerous security-critical packages including urllib3, starlette, keras, protobuf, numpy, and others to ensure system safety and reliability.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-bash"># Test the new GROUP BY WITH ROLLUP functionality
# This example demonstrates rollup operations on sales data

# Assuming you have a sales table with columns: region, product, amount

SELECT region, product, SUM(amount) as total_sales
FROM sales
GROUP BY WITH ROLLUP region, product;

# The output will include subtotals for each region and a grand total</code></pre>
    ]]></description>
    <guid isPermaLink="false">mindsdb/mindsdb-2026-02-25</guid>
    <pubDate>Wed, 25 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>agno - v2.5.5 - Slack Interface, ModelsLabTools, and Bug Fixes</title>
    <link>https://github.com/agno-agi/agno</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release introduces real-time streaming for Slack interfaces with live progress cards, adds image generation capabilities to ModelsLabTools, and includes several bug fixes for Gemini, AWS Bedrock, and workflow handling.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Slack Interface Streaming</strong>: Real-time streaming with live progress cards for tool calls, reasoning, and workflow steps. Multiple bots supported with separate tokens and signing secrets.</li>
<li><strong>ModelsLabTools Image Generation</strong>: Complete ModelsLab media suite now includes text-to-image API support for PNG/JPG generation.</li>
<li><strong>Gemini API Fix</strong>: Fixed empty message parts causing request failures when sending conversations to Gemini API.</li>
<li><strong>AWS Bedrock Tool Result Handling</strong>: Merged consecutive toolResult blocks into single user message for cleaner output.</li>
<li><strong>Workflow Image Handling</strong>: Fixed raw image bytes handling in workflow step's _convert_image_artifacts_to_images method.</li>
<li><strong>Knowledge Filter Serialization</strong>: Fixed FilterExpr object serialization in GET /agents and /teams responses.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from agno import Agent
from agno.tools import ModelsLabTools

# Example: Using ModelsLabTools with image generation
agent = Agent(
    tools=[ModelsLabTools()],
    instructions="Generate an image based on the description"
)

# Create an agent with Slack interface
from agno.interfaces import SlackInterface

slack_agent = Agent(
    tools=[ModelsLabTools()],
    interfaces=[SlackInterface(
        token="your-slack-bot-token",
        signing_secret="your-signing-secret"
    )]
)

# The Slack interface now provides real-time streaming with progress cards
# for tool calls, reasoning, and workflow steps</code></pre>
    ]]></description>
    <guid isPermaLink="false">agno-agi/agno-2026-02-25</guid>
    <pubDate>Wed, 25 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>ComfyUI - v0.15.0</title>
    <link>https://github.com/Comfy-Org/ComfyUI</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>ComfyUI v0.15.0 introduces new API nodes, audio processing capabilities, text generation support, and various bug fixes and improvements. Key additions include ElevenLabs nodes, a 3-band equalizer for audio, GLSL shader support, and essential subgraph blueprints.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>ElevenLabs API Nodes</strong>: Adds support for ElevenLabs text-to-speech and voice cloning capabilities through new API nodes.</li>
<li><strong>3-Band Equalizer Node</strong>: Provides audio processing capabilities with low, mid, and high frequency controls for audio enhancement.</li>
<li><strong>Text Generation with Native Models</strong>: Initial support for text generation using native models, starting with Gemma3 integration.</li>
<li><strong>GLSL Shader Node</strong>: Enables custom shader effects using PyOpenGL for advanced visual processing.</li>
<li><strong>Essential Subgraph Blueprints</strong>: Provides pre-built subgraph templates for common workflows to improve development efficiency.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python"># Example: Using ElevenLabs Text-to-Speech
from comfyui.api_nodes import ElevenLabsTTS

# Initialize with API key
tts = ElevenLabsTTS(api_key="your_api_key")

# Generate speech from text
speech_audio = tts.generate(
    text="Hello, this is a test of the ElevenLabs integration.",
    voice_id="male_1",
    speed=1.0
)

# Save the generated audio
speech_audio.save("output.wav")

# Example: Using 3-Band Equalizer
from comfyui.nodes import Equalizer3Band

# Apply audio equalization
equalizer = Equalizer3Band(low_gain=0.8, mid_gain=1.0, high_gain=1.2)
processed_audio = equalizer.apply(input_audio)</code></pre>
    ]]></description>
    <guid isPermaLink="false">Comfy-Org/ComfyUI-2026-02-24</guid>
    <pubDate>Tue, 24 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>docling - v2.75.0</title>
    <link>https://github.com/docling-project/docling</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release introduces unified model-family inference engines with KServe v2 API support and a new backend parser for XBRL instance reports. It also includes fixes for ASR segment handling and DOCX hyperlink address validation.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Unified Model-Family Inference Engines</strong>: Provides a consistent API for image classification and other model families, enabling easier integration with KServe v2 for production deployments.</li>
<li><strong>XBRL Instance Reports Parser</strong>: Adds native support for parsing XBRL (eXtensible Business Reporting Language) instance documents, enabling automated financial data extraction.</li>
<li><strong>DOCX Hyperlink Validation</strong>: Prevents crashes when processing DOCX files with missing hyperlink addresses, improving robustness for document processing pipelines.</li>
<li><strong>ASR Segment Filtering</strong>: Skips empty ASR (Automatic Speech Recognition) segments, reducing noise in speech-to-text processing workflows.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from docling import Document

# Parse XBRL instance report
xbrl_doc = Document.from_file("financial_report.xbrl")
print(xbrl_doc.metadata)

# Use unified inference engine for image classification
from docling.inference import ImageClassificationEngine
engine = ImageClassificationEngine.from_pretrained("hf_hub:model_id")
result = engine.predict(image_path="sample.jpg")
print(result)</code></pre>
    ]]></description>
    <guid isPermaLink="false">docling-project/docling-2026-02-24</guid>
    <pubDate>Tue, 24 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>langgraph - langgraph-sdk 0.3.9</title>
    <link>https://github.com/langchain-ai/langgraph</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release adds a new extract parameter to threads.search() and includes a make type target for type checking.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Enhanced Thread Search</strong>: Added extract parameter to threads.search() for more granular control over returned thread data</li>
<li><strong>Type Checking Support</strong>: Added make type target for improved type checking workflows</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-python">from langgraph_sdk import get_client

client = get_client()
# Using the new extract parameter to filter returned thread data
threads = await client.threads.search(extract=['thread_id', 'state', 'created_at'])
for thread in threads:
    print(f"Thread ID: {thread['thread_id']}, State: {thread['state']}")</code></pre>
    ]]></description>
    <guid isPermaLink="false">langchain-ai/langgraph-2026-02-24</guid>
    <pubDate>Tue, 24 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
  <item>
    <title>letta - v0.16.5</title>
    <link>https://github.com/letta-ai/letta</link>
    <description><![CDATA[
      <h3>üöÄ What's New</h3>
<p>This release contains a version bump to 0.16.5 with no functional changes or new features. It's a maintenance release that updates the version number.</p>
<h3>üí° Why It Matters</h3>
<ul>
<li><strong>Version Bump</strong>: Updated to v0.16.5 with no functional changes. This is a maintenance release for version tracking.</li>
</ul>
<h3>üõ†Ô∏è Try It Out</h3>
<pre><code class="language-bash"># No functional changes - this is just a version bump
# To verify your version:
letta --version
# Should output: 0.16.5</code></pre>
    ]]></description>
    <guid isPermaLink="false">letta-ai/letta-2026-02-24</guid>
    <pubDate>Tue, 24 Feb 2026 00:00:00 GMT</pubDate>
  </item>
  
</channel>
</rss>